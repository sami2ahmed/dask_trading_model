{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional packages I had to install to get this notebook working \n",
    "! pip install dask-ml\n",
    "! pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK imports\n",
    "from dask.distributed import Client\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "# Scikit imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# misc imports\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "# imports for loading AWS data \n",
    "import glob, os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from AWS at a months interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to run the bash below (depending on your permissioning) to get the data folder \n",
    "# created from the notebook itself \n",
    "\n",
    "# ! sudo chown -R jovyan:users ~/.local/share/jupyter \n",
    "! mkdir -p ~/data/deutsche-boerse-xetra-pds/{date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the start/end date and the output folders\n",
    "from_date = '2017-07-01'\n",
    "until_date = '2018-07-01'\n",
    "dates = list(pd.date_range(from_date, until_date, freq='M').strftime('%Y-%m-%d'))\n",
    "\n",
    "local_data_folder = '~/data/deutsche-boerse-xetra-pds' # do not end in /\n",
    "output_folder = '~/data/processed' # do not end in /\n",
    "\n",
    "! mkdir -p {local_data_folder}\n",
    "\n",
    "for date in dates:\n",
    "    success_file =  os.path.join(local_data_folder, date, 'success')\n",
    "    ! mkdir -p {local_data_folder}/{date}\n",
    "    ! aws s3 sync s3://deutsche-boerse-xetra-pds/{date} {local_data_folder}/{date} --no-sign-request\n",
    "    ! touch {success_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the cell above is too much data for you to pull \n",
    "# to get one day of data uncomment the bash below  \n",
    "\n",
    "# date = '2019-05-01'\n",
    "# ! aws s3 ls s3://deutsche-boerse-xetra-pds/{date}/ --no-sign-request \n",
    "# ! mkdir -p ~/data/deutsche-boerse-xetra-pds/{date}\n",
    "# ! aws s3 sync s3://deutsche-boerse-xetra-pds/{date} ~/data/deutsche-boerse-xetra-pds/{date} --no-sign-request\n",
    "# ! ls ~/data/deutsche-boerse-xetra-pds/{date}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will shove everything into a df like we usually would, and run some calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'data/db_data/{}' \n",
    "\n",
    "files = set()\n",
    "li_1 = []\n",
    "for i in dates:\n",
    "    file_name = path.format(i)\n",
    "    li_1.append(file_name)\n",
    "    for data_dir in li_1:\n",
    "        files.update(glob.glob(os.path.join(data_dir, '*.csv')))\n",
    "    big_df = pd.concat(map(pd.read_csv, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 32.3 ms, total: 188 ms\n",
      "Wall time: 191 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartPrice</th>\n",
       "      <th>MaxPrice</th>\n",
       "      <th>MinPrice</th>\n",
       "      <th>EndPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>456390.000000</td>\n",
       "      <td>456390.000000</td>\n",
       "      <td>456390.000000</td>\n",
       "      <td>456390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>67.843189</td>\n",
       "      <td>67.858359</td>\n",
       "      <td>67.827654</td>\n",
       "      <td>67.842840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.646164</td>\n",
       "      <td>96.659540</td>\n",
       "      <td>96.631026</td>\n",
       "      <td>96.643878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>44.220000</td>\n",
       "      <td>44.235000</td>\n",
       "      <td>44.200000</td>\n",
       "      <td>44.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>86.480000</td>\n",
       "      <td>86.500000</td>\n",
       "      <td>86.450000</td>\n",
       "      <td>86.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6481.500000</td>\n",
       "      <td>6483.000000</td>\n",
       "      <td>6481.500000</td>\n",
       "      <td>6483.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          StartPrice       MaxPrice       MinPrice       EndPrice\n",
       "count  456390.000000  456390.000000  456390.000000  456390.000000\n",
       "mean       67.843189      67.858359      67.827654      67.842840\n",
       "std        96.646164      96.659540      96.631026      96.643878\n",
       "min         0.003000       0.003000       0.003000       0.003000\n",
       "25%        20.000000      20.000000      19.990000      20.000000\n",
       "50%        44.220000      44.235000      44.200000      44.215000\n",
       "75%        86.480000      86.500000      86.450000      86.480000\n",
       "max      6481.500000    6483.000000    6481.500000    6483.000000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "big_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are computing the correlation between an indicator and rate of return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 106 ms, sys: 87.8 ms, total: 193 ms\n",
      "Wall time: 209 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PctChange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PctChange</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctChange[t - 1]</th>\n",
       "      <td>-0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indicator</th>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indicator[t - 1]</th>\n",
       "      <td>0.000211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PctChange\n",
       "PctChange          1.000000\n",
       "PctChange[t - 1]  -0.000384\n",
       "Indicator          0.001115\n",
       "Indicator[t - 1]   0.000211"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# calculate rate of return i.e. how much do the price changes compared to the previous price.\n",
    "big_df['PctChange'] = big_df['EndPrice'].pct_change()\n",
    "\n",
    "def line_distance(df, a, b):\n",
    "    return np.absolute(df[a] - df[b]) + np.absolute(df[a].shift(1) - df[b].shift(1))\n",
    "\n",
    "distance_to_max_line = line_distance(big_df, 'MaxPrice', 'EndPrice')\n",
    "distance_to_min_line = line_distance(big_df, 'MinPrice', 'EndPrice')\n",
    "\n",
    "big_df['Indicator'] = (distance_to_min_line - distance_to_max_line)/100.0 # divide by 100 because the prices are around 100\n",
    "big_df['Indicator[t - 1]'] =  big_df['Indicator'].shift(1)\n",
    "big_df['PctChange[t - 1]'] = big_df['PctChange'].shift(1)\n",
    "\n",
    "big_df[['PctChange',  'PctChange[t - 1]', 'Indicator', 'Indicator[t - 1]']].corr()[['PctChange']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, pandas is doing fine, given this data volume we're able to calculate rate of return etc. and are still performant. \n",
    "\n",
    "Changing gears, let's see how performance is beyond summary statistics. Let's try fitting a regressor on this data using regular Scikit and then using Dask to distribute joblib across a Saturn Dask Cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 456390 entries, 0 to 7077\n",
      "Data columns (total 18 columns):\n",
      "ISIN                456390 non-null object\n",
      "Mnemonic            456390 non-null object\n",
      "SecurityDesc        456390 non-null object\n",
      "SecurityType        456390 non-null object\n",
      "Currency            456390 non-null object\n",
      "SecurityID          456390 non-null object\n",
      "Date                456390 non-null object\n",
      "Time                456390 non-null object\n",
      "StartPrice          456390 non-null float64\n",
      "MaxPrice            456390 non-null float64\n",
      "MinPrice            456390 non-null float64\n",
      "EndPrice            456390 non-null float64\n",
      "TradedVolume        456390 non-null object\n",
      "NumberOfTrades      456390 non-null object\n",
      "PctChange           456389 non-null float64\n",
      "Indicator           456389 non-null float64\n",
      "Indicator[t - 1]    456388 non-null float64\n",
      "PctChange[t - 1]    456388 non-null float64\n",
      "dtypes: float64(8), object(10)\n",
      "memory usage: 66.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['ISIN', 'Mnemonic', 'SecurityDesc', 'SecurityType', 'Currency',\n",
       "       'SecurityID', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice',\n",
       "       'EndPrice', 'TradedVolume', 'NumberOfTrades', 'PctChange', 'Indicator',\n",
       "       'Indicator[t - 1]', 'PctChange[t - 1]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.info()\n",
    "big_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode SecurityType\n",
    "big_df['SecurityType'] = pd.get_dummies(big_df['SecurityType'])\n",
    "\n",
    "# change objects to floats \n",
    "big_df['TradedVolume'] = pd.to_numeric(big_df['TradedVolume'], errors='coerce')\n",
    "big_df['NumberOfTrades'] = pd.to_numeric(big_df['NumberOfTrades'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target from rest of dataset \n",
    "X = big_df.drop(['Time', 'Date','PctChange[t - 1]','PctChange','Mnemonic','EndPrice',\n",
    "               'SecurityDesc','Currency', 'ISIN'], axis=1)\n",
    "\n",
    "# X = big_df.drop(['PctChange[t - 1]','PctChange','EndPrice'], axis=1)\n",
    "\n",
    "y = big_df['PctChange']\n",
    "\n",
    "# split the data to have a true hold out dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=4444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputing',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='median',\n",
       "                               verbose=0)),\n",
       "                ('scaling',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('classifier',\n",
       "                 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                       criterion='mse', max_depth=None,\n",
       "                                       max_features='auto', max_leaf_nodes=None,\n",
       "                                       max_samples=None,\n",
       "                                       min_impurity_decrease=0.0,\n",
       "                                       min_impurity_split=None,\n",
       "                                       min_samples_leaf=1, min_samples_split=2,\n",
       "                                       min_weight_fraction_leaf=0.0,\n",
       "                                       n_estimators=100, n_jobs=None,\n",
       "                                       oob_score=False, random_state=None,\n",
       "                                       verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize our features\n",
    "standard_scaler = StandardScaler()\n",
    "# we will impute the median\n",
    "simple_imputer = SimpleImputer()\n",
    "# intiate random forest model\n",
    "log_reg = RandomForestRegressor()\n",
    "\n",
    "# setup pipeline to fit model on \n",
    "pl = Pipeline(steps=[\n",
    "    ('imputing', simple_imputer),\n",
    "    ('scaling', standard_scaler),\n",
    "    ('classifier', log_reg)\n",
    "])\n",
    "pl.set_params(\n",
    "    imputing__missing_values=np.nan, imputing__strategy=\"median\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 25s, sys: 1.29 s, total: 4min 26s\n",
      "Wall time: 4min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputing',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='median',\n",
       "                               verbose=0)),\n",
       "                ('scaling',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('classifier',\n",
       "                 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                       criterion='mse', max_depth=None,\n",
       "                                       max_features='auto', max_leaf_nodes=None,\n",
       "                                       max_samples=None,\n",
       "                                       min_impurity_decrease=0.0,\n",
       "                                       min_impurity_split=None,\n",
       "                                       min_samples_leaf=1, min_samples_split=2,\n",
       "                                       min_weight_fraction_leaf=0.0,\n",
       "                                       n_estimators=100, n_jobs=None,\n",
       "                                       oob_score=False, random_state=None,\n",
       "                                       verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is using regular scikit, no dask scaling out joblib to a cluster \n",
    "pl.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```4min 26s``` is not all that long, but we are only pulling a year of data from our dataset\n",
    "### let's see if we can do better switching over to Dask**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance using Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305a32c28c544451b8c1152f5df005b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SaturnCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup Dask Saturn cluster with 7 workers \n",
    "cluster = SaturnCluster()\n",
    "client = Client('tcp://sami-cancer-dask.main-namespace:8786')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note if you're trying to create a local cluster just use: ```client = Client(processes=False)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.99 s, sys: 5.56 s, total: 12.6 s\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# this is using dask to scale out joblib to a cluster \n",
    "with joblib.parallel_backend('dask'):\n",
    "    (pl.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```43.1 s``` is not a bad performance jump! \n",
    "### 84% speed up! \n",
    "### let's see how Dask stacks up on predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.28 s, sys: 31.9 ms, total: 8.31 s\n",
      "Wall time: 8.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#this is not using Dask, summarize the fit of the model\n",
    "y_predict = pl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 73.7 ms, total: 11.8 s\n",
      "Wall time: 6.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#this is using Dask \n",
    "with joblib.parallel_backend('dask'):\n",
    "    (pl.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as much of a performance gain but combined with the 617% performance gain during the fit process, saving a few additional seconds adds up to much faster training/predicting using this Dask+joblib distributed approach to financial modeling "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
